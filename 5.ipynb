{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvZCCalaAJ/rBtsT9DBWHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piotrwrzodak/UAM-LANG-MODELING/blob/main/5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "input_text = \"I live in New\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "for i in range(input_ids.shape[1]):\n",
        "  token_logits = logits[:, i, :]\n",
        "\n",
        "  probabilities = torch.nn.functional.softmax(token_logits, dim=-1)\n",
        "\n",
        "  top_k = 5\n",
        "  top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "  top_k_tokens = [tokenizer.decode([index]) for index in top_k_indices[0]]\n",
        "\n",
        "  print('')\n",
        "  for token, prob in zip(top_k_tokens, top_k_probs[0]):\n",
        "      print(f\"Token: {token} ; Probability: {prob.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCKke_PwY8pf",
        "outputId": "d40558a9-a5cb-4a5f-9584-24bb109e7ccc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token: . ; Probability: 0.041626300662755966\n",
            "Token: , ; Probability: 0.03326710686087608\n",
            "Token: \n",
            " ; Probability: 0.02399853803217411\n",
            "Token:  have ; Probability: 0.015221555717289448\n",
            "Token:  was ; Probability: 0.0142546147108078\n",
            "\n",
            "Token:  in ; Probability: 0.5987251996994019\n",
            "Token:  with ; Probability: 0.03872166946530342\n",
            "Token:  on ; Probability: 0.03659806400537491\n",
            "Token:  here ; Probability: 0.027593858540058136\n",
            "Token:  a ; Probability: 0.024963613599538803\n",
            "\n",
            "Token:  a ; Probability: 0.17659881711006165\n",
            "Token:  the ; Probability: 0.10188990086317062\n",
            "Token:  New ; Probability: 0.031843386590480804\n",
            "Token:  an ; Probability: 0.025642577558755875\n",
            "Token:  California ; Probability: 0.012568811886012554\n",
            "\n",
            "Token:  York ; Probability: 0.6568536758422852\n",
            "Token:  Jersey ; Probability: 0.12650558352470398\n",
            "Token:  Orleans ; Probability: 0.05511671304702759\n",
            "Token:  Hampshire ; Probability: 0.048315778374671936\n",
            "Token:  England ; Probability: 0.02394844777882099\n"
          ]
        }
      ]
    }
  ]
}